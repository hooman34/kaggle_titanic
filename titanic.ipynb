{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import Imputer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from pylab import *\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "%matplotlib inline\n",
    "# set default palette to Set2\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=plt.cm.Set2.colors)\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# when I want to specify the result of my print() function\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PassengerId as an index to use them as a key value\n",
    "train = pd.read_csv('train.csv', index_col = 'PassengerId')\n",
    "test = pd.read_csv('test.csv', index_col = 'PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['Survived']\n",
    "train = train.drop(['Survived'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## concat the two data sets, so it's easy to do feature engineering in one go\n",
    "titanic = pd.concat([train, test], keys=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save it for later submission\n",
    "passengerid = test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">train</th>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Pclass                                               Name  \\\n",
       "      PassengerId                                                              \n",
       "train 1                 3                            Braund, Mr. Owen Harris   \n",
       "      2                 1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "      3                 3                             Heikkinen, Miss. Laina   \n",
       "      4                 1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "      5                 3                           Allen, Mr. William Henry   \n",
       "\n",
       "                      Sex   Age  SibSp  Parch            Ticket     Fare  \\\n",
       "      PassengerId                                                          \n",
       "train 1              male  22.0      1      0         A/5 21171   7.2500   \n",
       "      2            female  38.0      1      0          PC 17599  71.2833   \n",
       "      3            female  26.0      0      0  STON/O2. 3101282   7.9250   \n",
       "      4            female  35.0      1      0            113803  53.1000   \n",
       "      5              male  35.0      0      0            373450   8.0500   \n",
       "\n",
       "                  Cabin Embarked  \n",
       "      PassengerId                 \n",
       "train 1             NaN        S  \n",
       "      2             C85        C  \n",
       "      3             NaN        S  \n",
       "      4            C123        S  \n",
       "      5             NaN        S  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the class 'engineering'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re as re\n",
    "\n",
    "\n",
    "class engineering():\n",
    "    \n",
    "    dock = titanic['Embarked'].value_counts().index.tolist()[0]\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def col_title(self):\n",
    "        def get_title(name):\n",
    "            title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "            # If the title exists, extract and return it.\n",
    "            if title_search:\n",
    "                return title_search.group(1)\n",
    "            return \"\"\n",
    "        self['Title'] = self['Name'].apply(get_title)\n",
    "        self['Title'] = self['Title'].replace('Mlle', 'Miss')\n",
    "        self['Title'] = self['Title'].replace('Sir', 'Mr')\n",
    "        self['Title'] = self['Title'].replace('Ms', 'Miss')\n",
    "        self['Title'] = self['Title'].replace('Mme', 'Mrs')\n",
    "        self['Title'] = self['Title'].replace(['Dr', 'Rev', 'Major', 'Col', 'Lady', 'Capt', 'Countess','Don', \n",
    "                                                     'Jonkheer', 'Dona'], 'Other')\n",
    "        return self\n",
    "    \n",
    "    def imp_age(self):\n",
    "        mid = []\n",
    "        # calculate the median, impute them\n",
    "        for title in titles:\n",
    "            a = self.Age[self['Title'] == title].median()\n",
    "            self.loc[:, 'Age'][(self['Title'] == title) & (self['Age'].isna())] = a\n",
    "        return self\n",
    "    \n",
    "    def imp_embark(self):\n",
    "        #dock = self.Embarked.value_counts().max()\n",
    "        self['Embarked'][self['Embarked'].isna()] = engineering.dock\n",
    "        return self\n",
    "    \n",
    "    def col_group(self):\n",
    "        self['Group_num'] = self[\"SibSp\"] + self[\"Parch\"]\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### making a 'Title' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are various titles in the 'Name' column, it should be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = engineering.col_title(titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNANs in data\u001b[0m \n",
      " \n",
      " Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age          263\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           1\n",
      "Cabin       1014\n",
      "Embarked       2\n",
      "Title          0\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check for missing values\n",
    "print(color.BOLD + \"NANs in data\" + color.END, \"\\n\", \"\\n\", titanic.isna().sum(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Survived' column is from the test dataset. The test and train datasets will be detatched after feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation - Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute the missing ages with the medians.\n",
    "<br>\n",
    "The medians are calculated with respect to title. \n",
    "<br>\n",
    "Mr, Miss, Master, etc will have different medians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have to set the title list, to make the function work.\n",
    "titles = ['Mr', \"Mrs\", 'Miss', 'Master', 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = engineering.imp_age(titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation - Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mEmbark Count\u001b[0m \n",
      " S    914\n",
      "C    270\n",
      "Q    123\n",
      "Name: Embarked, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# the missing values \n",
    "print(color.BOLD + 'Embark Count' + color.END, '\\n', titanic.Embarked.value_counts())\n",
    "# Will use the most common factor to impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = engineering.imp_embark(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the variable that you want to impute on column 'Embarked' by using the following code\n",
    "# As an example, you can do this\n",
    "# engineering.dock = titanic['Embarked'].value_counts().index.tolist()[1]\n",
    "\n",
    "# Confirm if it changed\n",
    "# engineering.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a 'group_num' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = engineering.col_group(titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_col = ['Name', 'Cabin', 'Ticket']\n",
    "titanic = titanic.drop(unused_col, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of feature engineering. Start scaling and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data sets into it's regular form\n",
    "train = titanic.ix['train']\n",
    "test  = titanic.ix['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definging the class for scaling and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "class scale_encode():\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def scaling(self):\n",
    "        # select numeric features\n",
    "        self = self.select_dtypes(exclude = [object])\n",
    "        # make it into an array, to feed it to the scaler\n",
    "        self = np.array(self)\n",
    "        scaler = preprocessing.StandardScaler().fit(self)\n",
    "        scaled = scaler.transform(self)\n",
    "\n",
    "        return scaled\n",
    "    \n",
    "    def encoding(self):\n",
    "        # select object features\n",
    "        self = self.select_dtypes(include= [object])\n",
    "        # factorizing the features\n",
    "        label_encode = preprocessing.LabelEncoder()\n",
    "        factor_encoded = self.apply(label_encode.fit_transform)\n",
    "        # define encoder, fit, and transform\n",
    "        encoder = preprocessing.OneHotEncoder()\n",
    "        encoder.fit(factor_encoded)\n",
    "        one_hot = encoder.transform(factor_encoded).toarray()\n",
    "        \n",
    "        return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Inspired by][1]\n",
    "\n",
    "[1]:http://www.ritchieng.com/machinelearning-one-hot-encoding/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing the numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There was one missing value in the test set. The column is 'Fare'. Impute median.\n",
    "test.loc[:, 'Fare'][test['Fare'].isna()] = median(test.loc[:, 'Fare'][test['Fare'].isna() == False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num_scaled = scale_encode.scaling(train)\n",
    "test_num_scaled = scale_encode.scaling(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHow it looks like\u001b[0m \n",
      " (891, 6) \n",
      " [[ 0.82737724 -0.55181621  0.43279337 -0.47367361 -0.50244517  0.05915988]\n",
      " [-1.56610693  0.65766815  0.43279337 -0.47367361  0.78684529  0.05915988]\n",
      " [ 0.82737724 -0.24944512 -0.4745452  -0.47367361 -0.48885426 -0.56097483]\n",
      " ...\n",
      " [ 0.82737724 -0.55181621  0.43279337  2.00893337 -0.17626324  1.29942929]\n",
      " [-1.56610693 -0.24944512 -0.4745452  -0.47367361 -0.04438104 -0.56097483]\n",
      " [ 0.82737724  0.20411152 -0.4745452  -0.47367361 -0.49237783 -0.56097483]]\n"
     ]
    }
   ],
   "source": [
    "print(color.BOLD + \"How it looks like\" + color.END, '\\n', \n",
    "      train_num_scaled.shape, '\\n', train_num_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot-encoding categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_factor_1hot = scale_encode.encoding(train)\n",
    "test_factor_1hot = scale_encode.encoding(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat the numerical and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the two arrays to make the final data set\n",
    "train_final = np.concatenate((train_num_scaled, train_factor_1hot), axis = 1)\n",
    "test_final = np.concatenate((test_num_scaled, test_factor_1hot), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThe final look of our data\u001b[0m \n",
      " (891, 16) \n",
      " [[ 0.82737724 -0.55181621  0.43279337 ...  1.          0.\n",
      "   0.        ]\n",
      " [-1.56610693  0.65766815  0.43279337 ...  0.          1.\n",
      "   0.        ]\n",
      " [ 0.82737724 -0.24944512 -0.4745452  ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.82737724 -0.55181621  0.43279337 ...  0.          0.\n",
      "   0.        ]\n",
      " [-1.56610693 -0.24944512 -0.4745452  ...  1.          0.\n",
      "   0.        ]\n",
      " [ 0.82737724  0.20411152 -0.4745452  ...  1.          0.\n",
      "   0.        ]]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(print(color.BOLD + \"The final look of our data\" + color.END, '\\n', \n",
    "      train_final.shape, '\\n', train_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start  ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# the gridsearch will search for 9 * 18 combinations of estimators\n",
    "params = [\n",
    "  {'C': [1, 5, 100], 'gamma': [1, 0.1, 0.001], 'kernel': ['rbf']},\n",
    "  {'C': [1, 5, 100], 'kernel': ['poly'], 'degree': [3, 4, 5], 'coef0': [1,2] }\n",
    " ]\n",
    "\n",
    "svc = svm.SVC(random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Searching\n"
     ]
    }
   ],
   "source": [
    "# do cross validation. Cross validation 5 times\n",
    "grid_search = GridSearchCV(svc, params, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(train_final, target)\n",
    "print(\"Done Searching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best parameters with out overfitting problems.\n",
    "a = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply grid - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8383838383838383"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# higher c means a stricter classifier.\n",
    "# rbf kernel adds similarity features\n",
    "# a small gamma value makes the bell-shaped curve wider, \n",
    "# so instances have a larger range of influence, and the decision boundary ends up smoother\n",
    "# So Î³ acts like a regularization hyperparameter: if your model is overfitting, you should reduce it\n",
    "\n",
    "model_svm = svm.SVC(kernel=\"rbf\", gamma=0.1, C=1, random_state = 10)\n",
    "model_svm.fit(train_final, target)\n",
    "model_svm.score(train_final, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bagging decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 300 models, trained on 100 random samples from the training data set.\n",
    "# bootstrap = True means the used obervations will be replaced\n",
    "# n_jobs=-1. Use all available CPU cores\n",
    "# oob_score. estimate out of bag score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8641975308641975"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=300,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "bag_clf.fit(train_final, target)\n",
    "bag_clf.score(train_final, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8361391694725028"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out of bag. It is the data observations that are not used when sampling, for each random sampling.\n",
    "# after the model is trained, it uses out of bag observations to predict.\n",
    "# since the oob observations are not used at training, they could give a glimpse of how the model will work on the real test data\n",
    "\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3. Randomforest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search - Randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rand_f = RandomForestClassifier()\n",
    "params = [\n",
    "    {\"n_estimators\" : [10, 100, 200, 300],\n",
    "    \"max_leaf_nodes\" : [5, 10, 15],\n",
    "    \"min_samples_leaf\" : [1, 2, 4]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Searching\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(rand_f, params, cv=5)\n",
    "grid_search.fit(train_final, target)\n",
    "print(\"Done Searching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_leaf_nodes': 10, 'min_samples_leaf': 1, 'n_estimators': 300}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply grid - Randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.835016835016835"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf = RandomForestClassifier(max_leaf_nodes= 10, min_samples_leaf= 1, n_estimators= 300)\n",
    "model_rf.fit(train_final, target)\n",
    "model_rf.score(train_final, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "extree = ExtraTreesClassifier()\n",
    "params = [\n",
    "    {\"n_estimators\" : [10, 100, 200, 300],\n",
    "    \"max_leaf_nodes\" : [10, 15, 20],\n",
    "    \"min_samples_leaf\" : [1, 2, 4]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(extree, params, cv=5)\n",
    "grid_search.fit(train_final, target)\n",
    "print('Done searching')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8305274971941639"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_et = ExtraTreesClassifier(max_leaf_nodes= 15, min_samples_leaf= 2, n_estimators= 10)\n",
    "model_et.fit(train_final, target)\n",
    "model_et.score(train_final, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "params = [\n",
    "    {\"learning_rate\": [0.05, 0.1, 0.5],\n",
    "     \"n_estimators\": [100, 200, 300],\n",
    "     \"max_depth\": [1, 3, 5]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(random_state = 10)\n",
    "grid_search = GridSearchCV(gb, params, cv=5, scoring='roc_auc', verbose = 3)\n",
    "grid_search.fit(train_final, target)\n",
    "print(\"Done searching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9315375982042648"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gb = GradientBoostingClassifier(learning_rate = 0.1, max_depth = 3, n_estimators = 200, random_state = 10)\n",
    "model_gb.fit(train_final, target)\n",
    "model_gb.score(train_final, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "params = [\n",
    "    {\"n_neighbors\": [5, 7, 10, 15]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Searching\n"
     ]
    }
   ],
   "source": [
    "kn = KNeighborsClassifier(n_jobs = -1)\n",
    "grid_search = GridSearchCV(kn, params, cv=5)\n",
    "grid_search.fit(train_final, target)\n",
    "print(\"Done Searching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 10}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8417508417508418"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_kn = KNeighborsClassifier(n_neighbors = 10)\n",
    "model_kn.fit(train_final, target)\n",
    "model_kn.score(train_final, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8092031425364759"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb = GaussianNB()\n",
    "model_nb.fit(train_final, target)\n",
    "model_nb.score(train_final, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the link to the stacking package \"vecstack\"\n",
    "<br>\n",
    "https://github.com/vecxoz/vecstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier()\n",
    "\n",
    "params = {\n",
    "        'n_estimators': [100, 250, 500],\n",
    "        'eta': [0.05, 0.1, 0.3],\n",
    "        'max_depth': [6, 9, 12],\n",
    "        'subsample': [0.9, 1.0],\n",
    "        'colsample_bytree': [0.9, 1.0],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter colsample_bytree for estimator GridSearchCV(cv=5, error_score='raise',\n       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n       n_jobs=1, nthread=4, num_class=2, objective='binary:logistic',\n       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n       seed=None, silent=1, subsample=1, val_metric='auc'),\n       fit_params=None, iid=True, n_jobs=1,\n       param_grid={'num_boost_round': [100, 250, 500], 'eta': [0.05, 0.1, 0.3], 'max_depth': [6, 9, 12], 'subsample': [0.9, 1.0], 'colsample_bytree': [0.9, 1.0], 'n_estimators': [100, 200]},\n       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n       scoring=None, verbose=0). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-894ca55769bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_final\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done Searching\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[0mtrain_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mset_params\u001b[1;34m(self, **params)\u001b[0m\n\u001b[0;32m    272\u001b[0m                                  \u001b[1;34m'Check the list of available parameters '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m                                  \u001b[1;34m'with `estimator.get_params().keys()`.'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m                                  (key, self))\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdelim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter colsample_bytree for estimator GridSearchCV(cv=5, error_score='raise',\n       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n       n_jobs=1, nthread=4, num_class=2, objective='binary:logistic',\n       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n       seed=None, silent=1, subsample=1, val_metric='auc'),\n       fit_params=None, iid=True, n_jobs=1,\n       param_grid={'num_boost_round': [100, 250, 500], 'eta': [0.05, 0.1, 0.3], 'max_depth': [6, 9, 12], 'subsample': [0.9, 1.0], 'colsample_bytree': [0.9, 1.0], 'n_estimators': [100, 200]},\n       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n       scoring=None, verbose=0). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(xg, params, cv=5)\n",
    "grid_search.fit(train_final, target)\n",
    "print(\"Done Searching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vecstack import StackingTransformer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Set the parameters according to the ones found above\n",
    "# Without gradientboost, the target gets better.\n",
    "\n",
    "estimators_L1 = [\n",
    "    ('et', ExtraTreesClassifier(max_leaf_nodes= 15, min_samples_leaf= 2, n_estimators= 10)),\n",
    "        \n",
    "    ('rf', RandomForestClassifier(max_leaf_nodes= 10, min_samples_leaf= 1, n_estimators= 100)),\n",
    "        \n",
    "    ('gb', GradientBoostingClassifier(learning_rate = 0.1, max_depth = 3, n_estimators = 200, random_state = 10)),\n",
    "    \n",
    "    ('sv', svm.SVC(kernel=\"rbf\", gamma=0.1, C=1, random_state = 10)),\n",
    "    \n",
    "    ('bc', BaggingClassifier(DecisionTreeClassifier(), n_estimators=300, max_samples=100, bootstrap=True, n_jobs=-1)),\n",
    "    \n",
    "    ('nb', GaussianNB()),\n",
    "    \n",
    "    ('kn', KNeighborsClassifier(n_neighbors = 10))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start stacking\n",
    "\n",
    "stack = StackingTransformer(estimators=estimators_L1,   # base estimators\n",
    "                            regression=False,           # regression task (if you need classification - set to False)\n",
    "                            variant='A',                # oof for train set, predict test set in each fold and find mean\n",
    "                            metric=mean_absolute_error, # metric: callable\n",
    "                            n_folds=4,                  # number of folds\n",
    "                            shuffle=True,               # shuffle the data\n",
    "                            random_state=10,            # ensure reproducibility\n",
    "                            verbose=2)                  # print all info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [2]\n",
      "metric:       [mean_absolute_error]\n",
      "variant:      [A]\n",
      "n_estimators: [4]\n",
      "\n",
      "estimator  0: [et: ExtraTreesClassifier]\n",
      "    fold  0:  [0.14798206]\n",
      "    fold  1:  [0.17488789]\n",
      "    fold  2:  [0.22869955]\n",
      "    fold  3:  [0.13963964]\n",
      "    ----\n",
      "    MEAN:     [0.17280229] + [0.03480179]\n",
      "\n",
      "estimator  1: [rf: RandomForestClassifier]\n",
      "    fold  0:  [0.14798206]\n",
      "    fold  1:  [0.16143498]\n",
      "    fold  2:  [0.22869955]\n",
      "    fold  3:  [0.13063063]\n",
      "    ----\n",
      "    MEAN:     [0.16718681] + [0.03715534]\n",
      "\n",
      "estimator  2: [sv: SVC]\n",
      "    fold  0:  [0.15695067]\n",
      "    fold  1:  [0.15695067]\n",
      "    fold  2:  [0.22869955]\n",
      "    fold  3:  [0.14414414]\n",
      "    ----\n",
      "    MEAN:     [0.17168626] + [0.03332926]\n",
      "\n",
      "estimator  3: [bc: BaggingClassifier]\n",
      "    fold  0:  [0.14349776]\n",
      "    fold  1:  [0.17488789]\n",
      "    fold  2:  [0.21973094]\n",
      "    fold  3:  [0.13063063]\n",
      "    ----\n",
      "    MEAN:     [0.16718681] + [0.03434282]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit the stacking transformer\n",
    "stack = stack.fit(train_final, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set was detected.\n",
      "Transforming...\n",
      "\n",
      "estimator  0: [et: ExtraTreesClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  1: [rf: RandomForestClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  2: [sv: SVC]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  3: [bc: BaggingClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "Transforming...\n",
      "\n",
      "estimator  0: [et: ExtraTreesClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  1: [rf: RandomForestClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  2: [sv: SVC]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  3: [bc: BaggingClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transform them\n",
    "stack1_train = stack.transform(train_final)\n",
    "stack1_test = stack.transform(test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done with the first layer.\n",
    "<br>\n",
    "We now have our train set and test set finalized. They are made based on the votes of our seven models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the correlations between models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking is like democracy. Models raise their hands if they think the person survived or not, and the result with the most votes will be finalized.\n",
    "<br>\n",
    "So, it is best to choose discrete models, to lower the correlation. With low correlation, there will be more improvements in the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\"ExtraTree\", \"RandomForest\", \"GradientBoost\", \"SVM\", \"Bagging\", \"NaiveBayes\", \"KNN\"]\n",
    "df = pd.DataFrame(stack1_train)\n",
    "df.columns = classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExtraTree</th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Bagging</th>\n",
       "      <th>NaiveBayes</th>\n",
       "      <th>KNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ExtraTree</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985149</td>\n",
       "      <td>0.957793</td>\n",
       "      <td>0.902756</td>\n",
       "      <td>0.917326</td>\n",
       "      <td>0.829722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.985149</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967799</td>\n",
       "      <td>0.913135</td>\n",
       "      <td>0.921977</td>\n",
       "      <td>0.845358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.957793</td>\n",
       "      <td>0.967799</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.895050</td>\n",
       "      <td>0.915004</td>\n",
       "      <td>0.862289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging</th>\n",
       "      <td>0.902756</td>\n",
       "      <td>0.913135</td>\n",
       "      <td>0.895050</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.858066</td>\n",
       "      <td>0.839184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaiveBayes</th>\n",
       "      <td>0.917326</td>\n",
       "      <td>0.921977</td>\n",
       "      <td>0.915004</td>\n",
       "      <td>0.858066</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.816814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.829722</td>\n",
       "      <td>0.845358</td>\n",
       "      <td>0.862289</td>\n",
       "      <td>0.839184</td>\n",
       "      <td>0.816814</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ExtraTree  RandomForest       SVM   Bagging  NaiveBayes  \\\n",
       "ExtraTree      1.000000      0.985149  0.957793  0.902756    0.917326   \n",
       "RandomForest   0.985149      1.000000  0.967799  0.913135    0.921977   \n",
       "SVM            0.957793      0.967799  1.000000  0.895050    0.915004   \n",
       "Bagging        0.902756      0.913135  0.895050  1.000000    0.858066   \n",
       "NaiveBayes     0.917326      0.921977  0.915004  0.858066    1.000000   \n",
       "KNN            0.829722      0.845358  0.862289  0.839184    0.816814   \n",
       "\n",
       "                   KNN  \n",
       "ExtraTree     0.829722  \n",
       "RandomForest  0.845358  \n",
       "SVM           0.862289  \n",
       "Bagging       0.839184  \n",
       "NaiveBayes    0.816814  \n",
       "KNN           1.000000  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the two links below, I was able to kick start stacking.\n",
    "<br>\n",
    "https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python\n",
    "<br>\n",
    "https://mlwave.com/kaggle-ensembling-guide/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8338945005611672"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model_xgb = XGBClassifier(colsample_bytree = 0.6, max_depth = 3, n_estimators = 1000, reg_alpha = 0.02, subsample = 1)\n",
    "model_xgb.fit(stack1_train, target)\n",
    "model_xgb.score(stack1_train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_xgb.predict(stack1_test)\n",
    "prediction = pd.DataFrame({\"PassengerId\": passengerid,\n",
    "                           \"Survived\": pred\n",
    "})\n",
    "prediction.to_csv(\"submit.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
